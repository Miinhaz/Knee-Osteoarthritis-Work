{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-01T08:28:39.274090Z","iopub.execute_input":"2023-07-01T08:28:39.274482Z","iopub.status.idle":"2023-07-01T08:28:50.397854Z","shell.execute_reply.started":"2023-07-01T08:28:39.274449Z","shell.execute_reply":"2023-07-01T08:28:50.396546Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from efficientnet) (1.0.8)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet) (0.20.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.8.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.10.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.5.0)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.28.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\nRequirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->scikit-image->efficientnet) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, Callback\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom efficientnet.keras import EfficientNetB5\nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:28:50.401757Z","iopub.execute_input":"2023-07-01T08:28:50.402072Z","iopub.status.idle":"2023-07-01T08:29:01.362903Z","shell.execute_reply.started":"2023-07-01T08:28:50.402044Z","shell.execute_reply":"2023-07-01T08:29:01.361837Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyCallback(Callback):\n    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n        super(MyCallback, self).__init__()\n        self.model = model\n        self.patience = patience\n        self.stop_patience = stop_patience\n        self.threshold = threshold\n        self.factor = factor\n        self.batches = batches\n        self.epochs = epochs\n        self.ask_epoch = ask_epoch\n        self.ask_epoch_initial = ask_epoch\n\n        self.count = 0\n        self.stop_count = 0\n        self.best_epoch = 1\n        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr))\n        self.highest_tracc = 0.0\n        self.lowest_vloss = np.inf\n        self.best_weights = self.model.get_weights()\n        self.initial_weights = self.model.get_weights()\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % self.batches == 0 and epoch > 0:\n            tr_acc = logs.get('accuracy')\n            v_loss = logs.get('val_loss')\n\n            if tr_acc > self.highest_tracc:\n                self.highest_tracc = tr_acc\n\n            if v_loss < self.lowest_vloss:\n                self.lowest_vloss = v_loss\n                self.best_epoch = epoch + 1\n                self.best_weights = self.model.get_weights()\n                self.count = 0\n            else:\n                self.count += 1\n\n            if tr_acc >= self.threshold:\n                self.ask_epoch -= 1\n\n            if self.count == self.patience and self.ask_epoch > 0:\n                print(\"\\nEpoch %d: Accuracy threshold reached. Decreasing learning rate.\" % (epoch + 1))\n                old_lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                new_lr = old_lr * self.factor\n                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n                print(\"Learning rate decreased from %f to %f\" % (old_lr, new_lr))\n                self.count = 0\n\n            if self.count == self.patience and self.ask_epoch == 0:\n                self.stop_count += 1\n                if self.stop_count == self.stop_patience:\n                    print(\"\\nTraining stopped at epoch %d\" % (epoch + 1))\n                    self.model.stop_training = True\n                else:\n                    print(\"\\nEpoch %d: Learning rate adjustment limit reached. Restoring best weights.\" % (epoch + 1))\n                    self.model.set_weights(self.best_weights)\n                    self.count = 0","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:29:01.364381Z","iopub.execute_input":"2023-07-01T08:29:01.365115Z","iopub.status.idle":"2023-07-01T08:29:01.380130Z","shell.execute_reply.started":"2023-07-01T08:29:01.365080Z","shell.execute_reply":"2023-07-01T08:29:01.379067Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Set the data path and categories\ndata_path = '/kaggle/input/kao-1500/MedicalExpert-I'\n\n# List all the categories (subfolders) in the data folder\ncategories = os.listdir(data_path)\n\n# Assign labels to the categories\nlabels = [i for i in range(len(categories))]\n\nlabel_dict = dict(zip(categories, labels))\n\nimg_size = 160\ndata = []\ntarget = []\n\n# Load the image data and labels\nfor category in categories:\n    folder_path = os.path.join(data_path, category)\n    img_names = os.listdir(folder_path)\n\n    for img_name in img_names:\n        img_path = os.path.join(folder_path, img_name)\n        img = cv2.imread(img_path)\n\n        try:\n            # Resize the image to the desired size\n            resized_img = cv2.resize(img, (img_size, img_size))\n            # Normalize the image\n            normalized_img = resized_img / 255.0\n            # Append the image and its corresponding label to the data and target lists\n            data.append(normalized_img)\n            target.append(label_dict[category])\n\n        except Exception as e:\n            print('Exception:', e)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:29:01.381656Z","iopub.execute_input":"2023-07-01T08:29:01.382314Z","iopub.status.idle":"2023-07-01T08:29:13.897639Z","shell.execute_reply.started":"2023-07-01T08:29:01.382277Z","shell.execute_reply":"2023-07-01T08:29:13.896193Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Convert the data and target lists to numpy arrays\ndata = np.array(data, dtype='float32')\ntarget = np.array(target)\n\n# Split the data into training, validation, and testing sets\nx_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n\n# Apply SMOTE oversampling to the training and validation sets\nsmote = SMOTE(random_state=42)\nx_train_res, y_train_res = smote.fit_resample(x_train.reshape(x_train.shape[0], -1), y_train)\nx_val_res, y_val_res = smote.fit_resample(x_val.reshape(x_val.shape[0], -1), y_val)\nx_train_res = x_train_res.reshape(x_train_res.shape[0], img_size, img_size, 3)\nx_val_res = x_val_res.reshape(x_val_res.shape[0], img_size, img_size, 3)\n\n# Convert the target arrays to categorical\ny_train_res = np_utils.to_categorical(y_train_res)\ny_val_res = np_utils.to_categorical(y_val_res)\n\n# Construct the EfficientNetB3 model\nbase_model = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3), pooling= 'max')\n\nmodel = Sequential([\n    base_model,\n    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n    Dropout(rate= 0.45, seed= 123),\n    Dense(len(categories), activation= 'softmax')\n])\n\nmodel.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:29:13.900737Z","iopub.execute_input":"2023-07-01T08:29:13.901340Z","iopub.status.idle":"2023-07-01T08:29:32.662669Z","shell.execute_reply.started":"2023-07-01T08:29:13.901306Z","shell.execute_reply":"2023-07-01T08:29:32.661685Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n115515256/115515256 [==============================] - 1s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n efficientnet-b5 (Functional  (None, 2048)             28513520  \n )                                                               \n                                                                 \n batch_normalization (BatchN  (None, 2048)             8192      \n ormalization)                                                   \n                                                                 \n dense (Dense)               (None, 256)               524544    \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n dense_1 (Dense)             (None, 5)                 1285      \n                                                                 \n=================================================================\nTotal params: 29,047,541\nTrainable params: 28,870,709\nNon-trainable params: 176,832\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Define the callbacks\ncheckpoint = ModelCheckpoint('/kaggle/working/best_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nmy_callback = MyCallback(model=model, patience=10, stop_patience=5, threshold=0.9, factor=0.1, batches=10, epochs=100, ask_epoch=10)\n\n# Train the model\nhistory = model.fit(x_train_res, y_train_res, batch_size=16, epochs=100, validation_data=(x_val_res, y_val_res), callbacks=[checkpoint, my_callback])","metadata":{"execution":{"iopub.status.busy":"2023-07-01T08:29:32.664482Z","iopub.execute_input":"2023-07-01T08:29:32.665299Z","iopub.status.idle":"2023-07-01T09:24:43.779198Z","shell.execute_reply.started":"2023-07-01T08:29:32.665260Z","shell.execute_reply":"2023-07-01T09:24:43.778120Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2023-07-01 08:30:16.859305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/efficientnet-b5/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"120/120 [==============================] - ETA: 0s - loss: 9.4452 - accuracy: 0.4157\nEpoch 1: val_accuracy improved from -inf to 0.27895, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 145s 311ms/step - loss: 9.4452 - accuracy: 0.4157 - val_loss: 8.8568 - val_accuracy: 0.2789\nEpoch 2/100\n120/120 [==============================] - ETA: 0s - loss: 7.6597 - accuracy: 0.6047\nEpoch 2: val_accuracy improved from 0.27895 to 0.54737, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 33s 278ms/step - loss: 7.6597 - accuracy: 0.6047 - val_loss: 7.1859 - val_accuracy: 0.5474\nEpoch 3/100\n120/120 [==============================] - ETA: 0s - loss: 6.4205 - accuracy: 0.6987\nEpoch 3: val_accuracy improved from 0.54737 to 0.63158, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 34s 282ms/step - loss: 6.4205 - accuracy: 0.6987 - val_loss: 6.1911 - val_accuracy: 0.6316\nEpoch 4/100\n120/120 [==============================] - ETA: 0s - loss: 5.3901 - accuracy: 0.7774\nEpoch 4: val_accuracy improved from 0.63158 to 0.69474, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 34s 279ms/step - loss: 5.3901 - accuracy: 0.7774 - val_loss: 5.2282 - val_accuracy: 0.6947\nEpoch 5/100\n120/120 [==============================] - ETA: 0s - loss: 4.5059 - accuracy: 0.8257\nEpoch 5: val_accuracy did not improve from 0.69474\n120/120 [==============================] - 31s 262ms/step - loss: 4.5059 - accuracy: 0.8257 - val_loss: 4.5047 - val_accuracy: 0.6737\nEpoch 6/100\n120/120 [==============================] - ETA: 0s - loss: 3.8213 - accuracy: 0.8635\nEpoch 6: val_accuracy improved from 0.69474 to 0.75789, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 34s 283ms/step - loss: 3.8213 - accuracy: 0.8635 - val_loss: 3.8305 - val_accuracy: 0.7579\nEpoch 7/100\n120/120 [==============================] - ETA: 0s - loss: 3.1483 - accuracy: 0.9155\nEpoch 7: val_accuracy did not improve from 0.75789\n120/120 [==============================] - 32s 264ms/step - loss: 3.1483 - accuracy: 0.9155 - val_loss: 3.3936 - val_accuracy: 0.7368\nEpoch 8/100\n120/120 [==============================] - ETA: 0s - loss: 2.6739 - accuracy: 0.9286\nEpoch 8: val_accuracy improved from 0.75789 to 0.77895, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 33s 277ms/step - loss: 2.6739 - accuracy: 0.9286 - val_loss: 2.8806 - val_accuracy: 0.7789\nEpoch 9/100\n120/120 [==============================] - ETA: 0s - loss: 2.2055 - accuracy: 0.9449\nEpoch 9: val_accuracy improved from 0.77895 to 0.81053, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 34s 282ms/step - loss: 2.2055 - accuracy: 0.9449 - val_loss: 2.4444 - val_accuracy: 0.8105\nEpoch 10/100\n120/120 [==============================] - ETA: 0s - loss: 1.8498 - accuracy: 0.9496\nEpoch 10: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 263ms/step - loss: 1.8498 - accuracy: 0.9496 - val_loss: 2.2508 - val_accuracy: 0.7316\nEpoch 11/100\n120/120 [==============================] - ETA: 0s - loss: 1.5441 - accuracy: 0.9591\nEpoch 11: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 266ms/step - loss: 1.5441 - accuracy: 0.9591 - val_loss: 1.8847 - val_accuracy: 0.8053\nEpoch 12/100\n120/120 [==============================] - ETA: 0s - loss: 1.2830 - accuracy: 0.9622\nEpoch 12: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 260ms/step - loss: 1.2830 - accuracy: 0.9622 - val_loss: 1.6458 - val_accuracy: 0.7789\nEpoch 13/100\n120/120 [==============================] - ETA: 0s - loss: 1.0915 - accuracy: 0.9580\nEpoch 13: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 260ms/step - loss: 1.0915 - accuracy: 0.9580 - val_loss: 1.5621 - val_accuracy: 0.7737\nEpoch 14/100\n120/120 [==============================] - ETA: 0s - loss: 0.9282 - accuracy: 0.9575\nEpoch 14: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.9282 - accuracy: 0.9575 - val_loss: 1.4163 - val_accuracy: 0.7737\nEpoch 15/100\n120/120 [==============================] - ETA: 0s - loss: 0.7534 - accuracy: 0.9806\nEpoch 15: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 259ms/step - loss: 0.7534 - accuracy: 0.9806 - val_loss: 1.3910 - val_accuracy: 0.7421\nEpoch 16/100\n120/120 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.9664\nEpoch 16: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.6802 - accuracy: 0.9664 - val_loss: 1.1787 - val_accuracy: 0.7684\nEpoch 17/100\n120/120 [==============================] - ETA: 0s - loss: 0.5801 - accuracy: 0.9696\nEpoch 17: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 259ms/step - loss: 0.5801 - accuracy: 0.9696 - val_loss: 1.2240 - val_accuracy: 0.7474\nEpoch 18/100\n120/120 [==============================] - ETA: 0s - loss: 0.5326 - accuracy: 0.9711\nEpoch 18: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.5326 - accuracy: 0.9711 - val_loss: 1.1896 - val_accuracy: 0.7263\nEpoch 19/100\n120/120 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.9675\nEpoch 19: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 263ms/step - loss: 0.4878 - accuracy: 0.9675 - val_loss: 1.0475 - val_accuracy: 0.7789\nEpoch 20/100\n120/120 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.9690\nEpoch 20: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.4481 - accuracy: 0.9690 - val_loss: 1.0701 - val_accuracy: 0.7632\nEpoch 21/100\n120/120 [==============================] - ETA: 0s - loss: 0.4034 - accuracy: 0.9701\nEpoch 21: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 267ms/step - loss: 0.4034 - accuracy: 0.9701 - val_loss: 1.0430 - val_accuracy: 0.7632\nEpoch 22/100\n120/120 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.9774\nEpoch 22: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 259ms/step - loss: 0.3645 - accuracy: 0.9774 - val_loss: 0.9367 - val_accuracy: 0.8053\nEpoch 23/100\n120/120 [==============================] - ETA: 0s - loss: 0.3267 - accuracy: 0.9795\nEpoch 23: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 260ms/step - loss: 0.3267 - accuracy: 0.9795 - val_loss: 1.1227 - val_accuracy: 0.7316\nEpoch 24/100\n120/120 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9801\nEpoch 24: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 260ms/step - loss: 0.3281 - accuracy: 0.9801 - val_loss: 1.0367 - val_accuracy: 0.7474\nEpoch 25/100\n120/120 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.9764\nEpoch 25: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 259ms/step - loss: 0.3223 - accuracy: 0.9764 - val_loss: 1.1086 - val_accuracy: 0.7474\nEpoch 26/100\n120/120 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.9822\nEpoch 26: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.2959 - accuracy: 0.9822 - val_loss: 1.0343 - val_accuracy: 0.7684\nEpoch 27/100\n120/120 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9822\nEpoch 27: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 264ms/step - loss: 0.2838 - accuracy: 0.9822 - val_loss: 0.9700 - val_accuracy: 0.7579\nEpoch 28/100\n120/120 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9806\nEpoch 28: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 265ms/step - loss: 0.2845 - accuracy: 0.9806 - val_loss: 0.8222 - val_accuracy: 0.7684\nEpoch 29/100\n120/120 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9801\nEpoch 29: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 31s 259ms/step - loss: 0.2634 - accuracy: 0.9801 - val_loss: 1.0091 - val_accuracy: 0.7474\nEpoch 30/100\n120/120 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9837\nEpoch 30: val_accuracy did not improve from 0.81053\n120/120 [==============================] - 32s 263ms/step - loss: 0.2560 - accuracy: 0.9837 - val_loss: 1.0724 - val_accuracy: 0.7000\nEpoch 31/100\n120/120 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9795\nEpoch 31: val_accuracy improved from 0.81053 to 0.82105, saving model to /kaggle/working/best_model.h5\n120/120 [==============================] - 34s 284ms/step - loss: 0.2632 - accuracy: 0.9795 - val_loss: 0.8036 - val_accuracy: 0.8211\nEpoch 32/100\n120/120 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9837\nEpoch 32: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.2530 - accuracy: 0.9837 - val_loss: 1.0685 - val_accuracy: 0.7316\nEpoch 33/100\n120/120 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9853\nEpoch 33: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.2373 - accuracy: 0.9853 - val_loss: 1.0392 - val_accuracy: 0.7526\nEpoch 34/100\n120/120 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9879\nEpoch 34: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.2277 - accuracy: 0.9879 - val_loss: 0.9849 - val_accuracy: 0.7632\nEpoch 35/100\n120/120 [==============================] - ETA: 0s - loss: 0.2216 - accuracy: 0.9869\nEpoch 35: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.2216 - accuracy: 0.9869 - val_loss: 0.9542 - val_accuracy: 0.7579\nEpoch 36/100\n120/120 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9837\nEpoch 36: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 257ms/step - loss: 0.2312 - accuracy: 0.9837 - val_loss: 0.9557 - val_accuracy: 0.8000\nEpoch 37/100\n120/120 [==============================] - ETA: 0s - loss: 0.2263 - accuracy: 0.9853\nEpoch 37: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.2263 - accuracy: 0.9853 - val_loss: 1.0852 - val_accuracy: 0.7632\nEpoch 38/100\n120/120 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9911\nEpoch 38: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.1932 - accuracy: 0.9911 - val_loss: 1.0317 - val_accuracy: 0.7474\nEpoch 39/100\n120/120 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9927\nEpoch 39: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.1918 - accuracy: 0.9927 - val_loss: 0.9462 - val_accuracy: 0.7632\nEpoch 40/100\n120/120 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9900\nEpoch 40: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1870 - accuracy: 0.9900 - val_loss: 1.0787 - val_accuracy: 0.7368\nEpoch 41/100\n120/120 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9885\nEpoch 41: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.2004 - accuracy: 0.9885 - val_loss: 1.1019 - val_accuracy: 0.7421\nEpoch 42/100\n120/120 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9858\nEpoch 42: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 262ms/step - loss: 0.1986 - accuracy: 0.9858 - val_loss: 1.1139 - val_accuracy: 0.7526\nEpoch 43/100\n120/120 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9921\nEpoch 43: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1771 - accuracy: 0.9921 - val_loss: 0.9213 - val_accuracy: 0.7789\nEpoch 44/100\n120/120 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9942\nEpoch 44: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1702 - accuracy: 0.9942 - val_loss: 0.9749 - val_accuracy: 0.7842\nEpoch 45/100\n120/120 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9858\nEpoch 45: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1909 - accuracy: 0.9858 - val_loss: 1.0432 - val_accuracy: 0.8053\nEpoch 46/100\n120/120 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9822\nEpoch 46: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.2064 - accuracy: 0.9822 - val_loss: 0.9015 - val_accuracy: 0.7947\nEpoch 47/100\n120/120 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9916\nEpoch 47: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1722 - accuracy: 0.9916 - val_loss: 1.0359 - val_accuracy: 0.7474\nEpoch 48/100\n120/120 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9864\nEpoch 48: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1850 - accuracy: 0.9864 - val_loss: 1.1664 - val_accuracy: 0.7526\nEpoch 49/100\n120/120 [==============================] - ETA: 0s - loss: 0.1778 - accuracy: 0.9895\nEpoch 49: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1778 - accuracy: 0.9895 - val_loss: 1.0039 - val_accuracy: 0.7474\nEpoch 50/100\n120/120 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9927\nEpoch 50: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1680 - accuracy: 0.9927 - val_loss: 1.0697 - val_accuracy: 0.7316\nEpoch 51/100\n120/120 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9879\nEpoch 51: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.1789 - accuracy: 0.9879 - val_loss: 1.1270 - val_accuracy: 0.7316\nEpoch 52/100\n120/120 [==============================] - ETA: 0s - loss: 0.1735 - accuracy: 0.9885\nEpoch 52: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1735 - accuracy: 0.9885 - val_loss: 1.0521 - val_accuracy: 0.7789\nEpoch 53/100\n120/120 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9927\nEpoch 53: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1594 - accuracy: 0.9927 - val_loss: 1.2299 - val_accuracy: 0.7263\nEpoch 54/100\n120/120 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9927\nEpoch 54: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1618 - accuracy: 0.9927 - val_loss: 0.8698 - val_accuracy: 0.8105\nEpoch 55/100\n120/120 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9921\nEpoch 55: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1587 - accuracy: 0.9921 - val_loss: 1.0212 - val_accuracy: 0.7632\nEpoch 56/100\n120/120 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9974\nEpoch 56: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 260ms/step - loss: 0.1481 - accuracy: 0.9974 - val_loss: 0.9992 - val_accuracy: 0.7789\nEpoch 57/100\n120/120 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9932\nEpoch 57: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1548 - accuracy: 0.9932 - val_loss: 1.1032 - val_accuracy: 0.7316\nEpoch 58/100\n120/120 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9937\nEpoch 58: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1599 - accuracy: 0.9937 - val_loss: 1.0368 - val_accuracy: 0.7684\nEpoch 59/100\n120/120 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9953\nEpoch 59: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1453 - accuracy: 0.9953 - val_loss: 0.9428 - val_accuracy: 0.7895\nEpoch 60/100\n120/120 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9921\nEpoch 60: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1517 - accuracy: 0.9921 - val_loss: 1.0910 - val_accuracy: 0.7684\nEpoch 61/100\n120/120 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9932\nEpoch 61: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1525 - accuracy: 0.9932 - val_loss: 0.9391 - val_accuracy: 0.7684\nEpoch 62/100\n120/120 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9969\nEpoch 62: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1313 - accuracy: 0.9969 - val_loss: 0.9881 - val_accuracy: 0.7474\nEpoch 63/100\n120/120 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9927\nEpoch 63: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1424 - accuracy: 0.9927 - val_loss: 1.0618 - val_accuracy: 0.7737\nEpoch 64/100\n120/120 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9969\nEpoch 64: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1360 - accuracy: 0.9969 - val_loss: 0.9311 - val_accuracy: 0.8105\nEpoch 65/100\n120/120 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9969\nEpoch 65: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1278 - accuracy: 0.9969 - val_loss: 0.8104 - val_accuracy: 0.8211\nEpoch 66/100\n120/120 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9932\nEpoch 66: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1420 - accuracy: 0.9932 - val_loss: 0.9994 - val_accuracy: 0.7684\nEpoch 67/100\n120/120 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9969\nEpoch 67: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1337 - accuracy: 0.9969 - val_loss: 0.9570 - val_accuracy: 0.7737\nEpoch 68/100\n120/120 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9963\nEpoch 68: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1327 - accuracy: 0.9963 - val_loss: 1.3279 - val_accuracy: 0.7263\nEpoch 69/100\n120/120 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9937\nEpoch 69: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1354 - accuracy: 0.9937 - val_loss: 1.0592 - val_accuracy: 0.7684\nEpoch 70/100\n120/120 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9953\nEpoch 70: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1289 - accuracy: 0.9953 - val_loss: 1.1917 - val_accuracy: 0.7632\nEpoch 71/100\n120/120 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9953\nEpoch 71: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 260ms/step - loss: 0.1272 - accuracy: 0.9953 - val_loss: 1.0409 - val_accuracy: 0.7789\nEpoch 72/100\n120/120 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9958\nEpoch 72: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 260ms/step - loss: 0.1307 - accuracy: 0.9958 - val_loss: 1.0507 - val_accuracy: 0.7842\nEpoch 73/100\n120/120 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9974\nEpoch 73: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1179 - accuracy: 0.9974 - val_loss: 1.0555 - val_accuracy: 0.7789\nEpoch 74/100\n120/120 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9921\nEpoch 74: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1332 - accuracy: 0.9921 - val_loss: 1.1326 - val_accuracy: 0.7526\nEpoch 75/100\n120/120 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9937\nEpoch 75: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 265ms/step - loss: 0.1356 - accuracy: 0.9937 - val_loss: 1.2535 - val_accuracy: 0.7526\nEpoch 76/100\n120/120 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9932\nEpoch 76: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1318 - accuracy: 0.9932 - val_loss: 1.4879 - val_accuracy: 0.7421\nEpoch 77/100\n120/120 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9963\nEpoch 77: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1276 - accuracy: 0.9963 - val_loss: 1.0685 - val_accuracy: 0.7579\nEpoch 78/100\n120/120 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9927\nEpoch 78: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1309 - accuracy: 0.9927 - val_loss: 1.0922 - val_accuracy: 0.7789\nEpoch 79/100\n120/120 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9937\nEpoch 79: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1346 - accuracy: 0.9937 - val_loss: 1.1974 - val_accuracy: 0.7474\nEpoch 80/100\n120/120 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9969\nEpoch 80: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1188 - accuracy: 0.9969 - val_loss: 1.1307 - val_accuracy: 0.7789\nEpoch 81/100\n120/120 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9942\nEpoch 81: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1253 - accuracy: 0.9942 - val_loss: 1.3613 - val_accuracy: 0.7684\nEpoch 82/100\n120/120 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9974\nEpoch 82: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1174 - accuracy: 0.9974 - val_loss: 1.0001 - val_accuracy: 0.7842\nEpoch 83/100\n120/120 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9979\nEpoch 83: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1123 - accuracy: 0.9979 - val_loss: 1.1727 - val_accuracy: 0.7632\nEpoch 84/100\n120/120 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9958\nEpoch 84: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1198 - accuracy: 0.9958 - val_loss: 0.9582 - val_accuracy: 0.7737\nEpoch 85/100\n120/120 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9958\nEpoch 85: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1142 - accuracy: 0.9958 - val_loss: 0.9632 - val_accuracy: 0.7421\nEpoch 86/100\n120/120 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9953\nEpoch 86: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 258ms/step - loss: 0.1146 - accuracy: 0.9953 - val_loss: 0.9867 - val_accuracy: 0.7947\nEpoch 87/100\n120/120 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9942\nEpoch 87: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1167 - accuracy: 0.9942 - val_loss: 1.1992 - val_accuracy: 0.7632\nEpoch 88/100\n120/120 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9958\nEpoch 88: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1107 - accuracy: 0.9958 - val_loss: 0.9948 - val_accuracy: 0.7947\nEpoch 89/100\n120/120 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9948\nEpoch 89: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1141 - accuracy: 0.9948 - val_loss: 1.0379 - val_accuracy: 0.7789\nEpoch 90/100\n120/120 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9953\nEpoch 90: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 265ms/step - loss: 0.1169 - accuracy: 0.9953 - val_loss: 1.0265 - val_accuracy: 0.8000\nEpoch 91/100\n120/120 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9911\nEpoch 91: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 260ms/step - loss: 0.1188 - accuracy: 0.9911 - val_loss: 1.1229 - val_accuracy: 0.7579\nEpoch 92/100\n120/120 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9953\nEpoch 92: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 262ms/step - loss: 0.1198 - accuracy: 0.9953 - val_loss: 0.9547 - val_accuracy: 0.8000\nEpoch 93/100\n120/120 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9969\nEpoch 93: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1076 - accuracy: 0.9969 - val_loss: 0.9426 - val_accuracy: 0.7842\nEpoch 94/100\n120/120 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9958\nEpoch 94: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1105 - accuracy: 0.9958 - val_loss: 1.0672 - val_accuracy: 0.7737\nEpoch 95/100\n120/120 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9984\nEpoch 95: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1046 - accuracy: 0.9984 - val_loss: 1.0801 - val_accuracy: 0.7737\nEpoch 96/100\n120/120 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9969\nEpoch 96: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 263ms/step - loss: 0.1043 - accuracy: 0.9969 - val_loss: 1.0408 - val_accuracy: 0.7579\nEpoch 97/100\n120/120 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9979\nEpoch 97: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 262ms/step - loss: 0.1028 - accuracy: 0.9979 - val_loss: 0.8802 - val_accuracy: 0.7737\nEpoch 98/100\n120/120 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9958\nEpoch 98: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 31s 259ms/step - loss: 0.1039 - accuracy: 0.9958 - val_loss: 0.8975 - val_accuracy: 0.8000\nEpoch 99/100\n120/120 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9948\nEpoch 99: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1136 - accuracy: 0.9948 - val_loss: 1.0365 - val_accuracy: 0.7579\nEpoch 100/100\n120/120 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9979\nEpoch 100: val_accuracy did not improve from 0.82105\n120/120 [==============================] - 32s 264ms/step - loss: 0.1010 - accuracy: 0.9979 - val_loss: 1.2138 - val_accuracy: 0.7158\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the best weights\nmodel.load_weights('/kaggle/working/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:24:43.781160Z","iopub.execute_input":"2023-07-01T09:24:43.781802Z","iopub.status.idle":"2023-07-01T09:24:44.461118Z","shell.execute_reply.started":"2023-07-01T09:24:43.781756Z","shell.execute_reply":"2023-07-01T09:24:44.460027Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Convert y_test to one-hot encoded format\nnum_classes = len(categories)\ny_test_encoded = np_utils.to_categorical(y_test, num_classes)\n\n# Evaluate the model on the test set\nval_loss, val_accuracy = model.evaluate(x_test, y_test_encoded, verbose=0)\nprint(\"Test loss:\", val_loss)\nprint(\"Test accuracy:\", val_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:30:22.328594Z","iopub.execute_input":"2023-07-01T09:30:22.328976Z","iopub.status.idle":"2023-07-01T09:30:25.446936Z","shell.execute_reply.started":"2023-07-01T09:30:22.328947Z","shell.execute_reply":"2023-07-01T09:30:25.445974Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Test loss: 1.0148918628692627\nTest accuracy: 0.7757575511932373\n","output_type":"stream"}]},{"cell_type":"code","source":"# Plot the training loss and accuracy\nN = len(history.history['loss'])\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"center right\")\nplt.savefig(\"EfficientNet_Model\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:24:48.012779Z","iopub.status.idle":"2023-07-01T09:24:48.013530Z","shell.execute_reply.started":"2023-07-01T09:24:48.013237Z","shell.execute_reply":"2023-07-01T09:24:48.013274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the training and validation accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot the training and validation loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:24:48.015439Z","iopub.status.idle":"2023-07-01T09:24:48.016562Z","shell.execute_reply.started":"2023-07-01T09:24:48.016252Z","shell.execute_reply":"2023-07-01T09:24:48.016290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test data\ny_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n#Generate and plot the confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplot_confusion_matrix(conf_mat=cm, class_names=categories, show_normed=True, figsize=(10, 10))\nplt.title('Confusion Matrix')\nplt.show()\n\n# Print the classification report\nreport = classification_report(y_true, y_pred, target_names=categories)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T09:24:48.018272Z","iopub.status.idle":"2023-07-01T09:24:48.018785Z","shell.execute_reply.started":"2023-07-01T09:24:48.018532Z","shell.execute_reply":"2023-07-01T09:24:48.018557Z"},"trusted":true},"execution_count":null,"outputs":[]}]}